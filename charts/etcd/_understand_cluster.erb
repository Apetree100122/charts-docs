<%
=begin
apps: etcd
platforms: kubernetes
id: understand_cluster
title: Understand cluster bootstrapping and scalability
category: configuration
weight: 40
highlight: 40
=end %>

### Cluster bootstrapping

The Bitnami etcd chart uses the **static discovery configured via environment variables** to bootstrap the etcd cluster. Based on the number of initial replicas, and using the A records added to the DNS configuration by the headless service, the chart can calculate every advertised peer URL.
The chart makes use of a couple of extra elements offered by Kubernetes to ensure the bootstrapping is successful:

- Setting a "Parallel" Pod Management Policy. This is critical, since all the etcd replicas should be created simultaneously to guarantee they can find each other.
- Recording not ready pods in the DNS, so etcd replicas are reachable using their associated FQDN before they're actually ready.

> Ref: [etcd discovery](https://etcd.io/docs/current/op-guide/clustering/#discovery)
> Ref: [Pod Management Policies](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies)
> Ref: [Recording not ready pods](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-hostname-and-subdomain-fields)

With this in mind, let's see an example of the environment configuration bootstrapping an etcd cluster with 3 replicas:

| Member  | Variable                         | Value                                                                                                                                                                                                 |
|---------|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 0       | ETCD_NAME                        | etcd-0                                                                                                                                                                                                |
| 0       | ETCD_INITIAL_ADVERTISE_PEER_URLS | http://etcd-1.etcd-headless.default.svc.cluster.local:2380                                                                                                                                            |
|---------|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1       | ETCD_NAME                        | etcd-1                                                                                                                                                                                                |
| 1       | ETCD_INITIAL_ADVERTISE_PEER_URLS | http://etcd-1.etcd-headless.default.svc.cluster.local:2380                                                                                                                                            |
|---------|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 2       | ETCD_NAME                        | etcd-2                                                                                                                                                                                                |
| 2       | ETCD_INITIAL_ADVERTISE_PEER_URLS | http://etcd-2.etcd-headless.default.svc.cluster.local:2380                                                                                                                                            |
|---------|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| *       | ETCD_INITIAL_CLUSTER_STATE       | new                                                                                                                                                                                                   |
| *       | ETCD_INITIAL_CLUSTER_TOKEN       | etcd-cluster-k8s                                                                                                                                                                                      |
| *       | ETCD_INITIAL_CLUSTER             | etcd-0=http://etcd-0.etcd-headless.default.svc.cluster.local:2380,etcd-1=http://etcd-1.etcd-headless.default.svc.cluster.local:2380,etcd-2=http://etcd-2.etcd-headless.default.svc.cluster.local:2380 |

The probes (readiness & liveness) are delayed 60 seconds by default, to give time to the etcd replicas to start and find each other. After that period, the *etcdctl endpoint health* command is used to periodically perform health checks on every replica.

### Scaling the cluster

The Bitnami etcd chart uses the etcd reconfiguration operations add/remove members of the cluster during scales.

When scaling down, a "pre-stop" lifecycle hook is used to ensure the *etcdctl member remove* command is executed and it stores the output of running this command in the Persistent Volume attached to the etcd pod. This hook is also executed when the pod is manually removed using the *kubectl delete pod** command or rescheduled by K8s for whatever reason, which implies that the cluster can be scaled up/down without human intervention. Let's as an example of this works:

1. We bootstrap an etcd cluster with 3 members running on a 3 nodes k8s cluster.
1. After some days, the k8s cluster admin decides to upgrade the kernel on one of the K8s nodes. To do so, the administrator drains the node which implies that pods running on that node are rescheduled to a different one.
1. During the pod eviction process, the "pre-stop" hook will remove the etcd member from the cluster, so the etcd cluster is scaled down to only 2 members.
1. Once the pod is scheduled in another k8s node and initialized, the etcd member is added again to the cluster using the *etcdctl member add* command. This way, the etcd cluster is scaled up to 3 replicas.

If, for whatever reason, the "pre-stop" hook fails removing the member, the initialization logic is able to detect something went wrong checking the *etcdctl member remove* command output that was stored in the persistent volume. When this happens, it uses the *etcdctl member update* command to rejoin the member. In this case, the cluster isn't automatically scaled down/up while the pod is recovered and other members will attempt to connect to the pod causing some warnings/errors like the one below:

~~~
E | rafthttp: failed to dial XXXXXXXX on stream Message (peer XXXXXXXX failed to find local node YYYYYYYYY)
I | rafthttp: peer XXXXXXXX became inactive (message send to peer failed)
W | rafthttp: health check for peer XXXXXXXX could not connect: dial tcp A.B.C.D:2380: i/o timeout
~~~

> Ref: [etcd runtime configuration](https://etcd.io/docs/current/op-guide/runtime-configuration/)
> Ref: [Safely drain a K8s node](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/)

## Updating the cluster

When updating the etcd statefulset (e.g. when upgrading the chart version via *helm upgrade* command), every pod must be replaced following the statefulset update strategy.
The chart uses by default a "RollingUpdate" strategy with default K8s values. In other words, updating each Pod one at a time in the same order as Pod termination (from the largest ordinal to the smallest). It will wait until an updated Pod is Running and Ready prior to updating its predecessor.

> Ref: [Update Strategies](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies)

### Disaster recovery

If, for whatever reason, *(N-1)/2* members are down and the "pre-stop" hooks fail removing them from the cluster, the cluster disastrously fails, irrevocably losing quorum. Once quorum is lost, the cluster cannot reach consensus and therefore cannot continue accepting updates. Under this circumstance, there's only one possible solution, restoring the cluster from a snapshot.

> Important: all members should restore using the same snapshot.

The Bitnami etcd chart solves this problem optionally offering a K8s cronjob that periodically snapshots the keyspace and stores it in a RWX volume. In case the cluster disastrously fails, the pods will automatically try to restore it using the last taken snapshot.

Find detailed instructions to enable this feature [on this section](https://docs.bitnami.com/kubernetes/infrastructure/etcd/administration/enable-disaster-recovery).

The chart also sets by default a "soft" Pod AntiAffinity to reduce the risks for the cluster to disastrously fail.

> Ref: [etcd recovery](https://etcd.io/docs/current/op-guide/recovery)
> Ref: [K8s Cronjobs](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/)
> Ref: [Affinity and anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)
